from fastapi import FastAPI, Request, Form
from fastapi.responses import HTMLResponse, JSONResponse, StreamingResponse
from fastapi.templating import Jinja2Templates
from openai import OpenAI
from dotenv import load_dotenv
import os, asyncio, json

from jpsearch_client import search_japan_search_by_author

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise RuntimeError("OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

openai_client = OpenAI(api_key=api_key)

# FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
app = FastAPI()
templates = Jinja2Templates(directory="templates")

# ã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹ï¼ˆç°¡æ˜“ï¼‰
app.state.current_author = ""
app.state.ref_items = []

# -------------------------------
# ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ï¼ˆãƒ¢ãƒ¼ãƒ‰é¸æŠï¼‰
# -------------------------------
@app.get("/", response_class=HTMLResponse)
async def index(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

# -------------------------------
# æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰
# -------------------------------
@app.get("/search", response_class=HTMLResponse)
async def search_page(request: Request):
    return templates.TemplateResponse("search.html", {
        "request": request,
        "results": None,
        "author": None,
        "error": None
    })

@app.post("/search", response_class=HTMLResponse)
async def search(request: Request, author: str = Form(...)):
    author = author.strip()
    if not author:
        return templates.TemplateResponse("search.html", {
            "request": request,
            "results": [],
            "author": "",
            "error": "è‘—è€…åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚"
        })

    try:
        results = search_japan_search_by_author(author)
        if results:
            app.state.current_author = author
            app.state.ref_items = [{"title": r["title"], "date": r["date"]} for r in results[:3]]
        else:
            app.state.current_author = ""
            app.state.ref_items = []
            return templates.TemplateResponse("search.html", {
                "request": request,
                "results": [],
                "author": author,
                "error": "è©²å½“ã™ã‚‹è³‡æ–™ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
            })

        return templates.TemplateResponse("search.html", {
            "request": request,
            "results": results,
            "author": author,
            "error": None
        })

    except Exception as e:
        print(f"[æ¤œç´¢ã‚¨ãƒ©ãƒ¼] {e}")
        return templates.TemplateResponse("search.html", {
            "request": request,
            "results": [],
            "author": author,
            "error": "æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
        })

# -------------------------------
# è§£èª¬ã‚¹ãƒˆãƒªãƒ¼ãƒ ï¼ˆOpenAI + æ¤œç´¢çµæœï¼‰
# -------------------------------
@app.get("/stream")
async def stream():
    author = app.state.current_author
    ref_items = app.state.ref_items

    if not author or not ref_items:
        return JSONResponse({"error": "æ¤œç´¢çµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚å†åº¦æ¤œç´¢ã—ã¦ãã ã•ã„ã€‚"}, status_code=400)

    refs_md = "\n".join([f"- {itm['title']}ï¼ˆ{itm['date']}ï¼‰" for itm in ref_items])

    prompt = f"""
# æŒ‡ç¤º
æ–‡å­¦ä½œå“ã‚„è‘—æ›¸ã«é–¢ã™ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å½¹ã«ç«‹ã¤æƒ…å ±ã‚’ä¼ãˆã‚‹ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã§ã™ã€‚
ã‚¸ãƒ£ãƒ‘ãƒ³ã‚µãƒ¼ãƒã§è‘—è€…ã§æ¤œç´¢ã—ãŸã¨ãã®çµæœä¸Šä½3ä»¶ã®å‚è€ƒä½œå“ã‚’å‚ç…§ã—ã¦ã€æ¤œç´¢ãƒ¯ãƒ¼ãƒ‰ã«åŸºã¥ã„ãŸç°¡æ½”ãªèª¬æ˜ã‚’ã—ã¦ä¸‹ã•ã„ã€‚

# åˆ¶ç´„æ¡ä»¶
- å‡ºåŠ›ã¯300å­—ä»¥å†…
- è¦ªã—ã¿ã‚„ã™ã„å£èª¿ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼æ¥ã—ã¦ãã ã•ã„ã€‚
- å¿…ãšæœ€å¾Œã«æ¬¡ã®è‘—è€…ã®æ¤œç´¢ã®ææ¡ˆã‚’è¡Œã£ã¦ãã ã•ã„ã€‚
- æ®µè½ã‚’æ˜ç¢ºã«åˆ†ã‘ã¦ãã ã•ã„ï¼ˆç©ºè¡Œã‚’å«ã‚ã¦ï¼‰
- æ®µè½ãŒå¤‰ã‚ã£ãŸå ´åˆã¯å¿…ãšè¦‹å‡ºã—ã‚’ã¤ã‘ã¦ãã ã•ã„ã€‚
- å®Œå…¨ãªãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§æ›¸ã„ã¦ãã ã•ã„
- å‚è€ƒä½œå“ãŒãªã„ã¨ãã¯ãã®æ—¨ã‚’ä¼ãˆãŸä¸Šã§å½¹ã«ç«‹ã¤å›ç­”ã‚’ã—ã¦ãã ã•ã„ã€‚

# æ¤œç´¢ãƒ¯ãƒ¼ãƒ‰
{author}

# å‚è€ƒä½œå“
{refs_md}
"""

    def run_sync():
        return openai_client.chat.completions.create(
            model="gpt-4o",
            stream=True,
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯æ–‡åŒ–è§£èª¬è€…ã§ã™ã€‚Markdownå½¢å¼ã§ã‚ã‹ã‚Šã‚„ã™ãå‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.4,
        )

    async def generator():
        try:
            loop = asyncio.get_event_loop()
            stream = await loop.run_in_executor(None, run_sync)
            for chunk in stream:
                text = getattr(chunk.choices[0].delta, "content", "")
                if text:
                    yield f"data: {json.dumps({'text': text})}\n\n"
                await asyncio.sleep(0)
        except Exception as e:
            yield f"data: {json.dumps({'error': str(e)})}\n\n"

        yield f"data: {json.dumps({'done': True})}\n\n"

    return StreamingResponse(generator(), media_type="text/event-stream")

# -------------------------------
# ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰ï¼ˆå¯¾è©±ãƒ™ãƒ¼ã‚¹æ¤œç´¢ï¼‰
# -------------------------------
@app.get("/chat", response_class=HTMLResponse)
async def chat_page(request: Request):
    return templates.TemplateResponse("chat.html", {"request": request})

@app.post("/chat-stream")
async def chat_stream(request: Request):
    try:
        data = await request.json()
        history = data.get("history", [])

        print("\nğŸ“¥ USER INPUT:")
        for h in history:
            print(f"{h['role']}: {h['content']}")

        # ğŸ¯ ä¼šè©±ç”Ÿæˆã®ãŸã‚ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆsystemå«ã‚€ï¼‰
        messages = [
            {"role": "system", "content": """# æŒ‡ç¤º
ã‚ãªãŸã¯æ–‡å­¦ä½œå“ã‚„è‘—æ›¸ã«é–¢ã™ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å½¹ã«ç«‹ã¤æƒ…å ±ã‚’ä¼ãˆã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ã‚¸ãƒ£ãƒ‘ãƒ³ã‚µãƒ¼ãƒã§æ¤œç´¢ãƒ¯ãƒ¼ãƒ‰ã‚’ãŠå®¢ã•ã‚“ã®ãƒ‹ãƒ¼ã‚ºã‹ã‚‰ææ¡ˆã—ã¦ãã ã•ã„ã€‚

# åˆ¶ç´„æ¡ä»¶
- å‡ºåŠ›ã¯100å­—ä»¥å†…
- è¦ªã—ã¿ã‚„ã™ã„å£èª¿ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«æ¥ã—ã¦ãã ã•ã„
- ç›¸æ‰‹ã®ãƒ‹ãƒ¼ã‚ºãŒåˆ†ã‹ã‚‰ãªã„å ´åˆã¯ã‚¸ãƒ£ãƒ³ãƒ«ç­‰ã‚’ã“ã¡ã‚‰ã‹ã‚‰ææ¡ˆã—ã¦ãã ã•ã„ã€‚
- ä½†ã—ã€ç„¡ç†ã«è‘—è€…ã‚’ææ¡ˆã™ã‚‹ã®ã¯ä¸è‡ªç„¶ãªã®ã§ã€ã‚¤ãƒ¡ãƒ¼ã‚¸ã§ããªã„å ´åˆã¯æ·±æ˜ã‚Šã‚’è¡Œã£ã¦ãã ã•ã„ã€‚
- æ®µè½ã‚’æ˜ç¢ºã«åˆ†ã‘ã¦ãã ã•ã„ï¼ˆç©ºè¡Œã‚’å«ã‚ã¦ï¼‰
- æ®µè½ãŒå¤‰ã‚ã£ãŸå ´åˆã¯å¿…ãšè¦‹å‡ºã—ã‚’ã¤ã‘ã¦ãã ã•ã„
- å®Œå…¨ãªãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§æ›¸ã„ã¦ãã ã•ã„
"""}
        ]

        for msg in history:
            messages.append({"role": msg["role"], "content": msg["content"]})

        messages.append({
            "role": "user",
            "content": """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®èˆˆå‘³ã«åŸºã¥ã„ã¦ã€å¿…ãšå…·ä½“çš„ãªè‘—è€…åã‚’1åä»¥ä¸Šææ¡ˆã—ã¦ãã ã•ã„ã€‚
è‘—è€…åã¯æ¤œç´¢å¯èƒ½ãªå®Ÿåœ¨ã®æ—¥æœ¬ã®ä½œå®¶ãƒ»æ–‡å­¦è€…ã«é™å®šã—ã€ãã®äººã®ä»£è¡¨ä½œã‚‚ä½µã›ã¦ç´¹ä»‹ã—ã¦ãã ã•ã„ã€‚
æ¤œç´¢çµæœãŒã¾ã ãªã„å ´åˆã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®èˆˆå‘³é–¢å¿ƒã‚’ãµã¾ãˆã¦æ¤œç´¢ã«ã¤ãªãŒã‚‹è‘—è€…ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚"""
        })

        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”¨ã®åŒæœŸé–¢æ•°
        def run_sync():
            return openai_client.chat.completions.create(
                model="gpt-4o",
                stream=True,
                messages=messages,
                temperature=0.5,
            )

        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼
        async def generator():
            collected_message = ""
            try:
                # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹é–‹å§‹
                loop = asyncio.get_event_loop()
                stream = await loop.run_in_executor(None, run_sync)

                for chunk in stream:
                    text = getattr(chunk.choices[0].delta, "content", "")
                    if text:
                        collected_message += text
                        yield f"data: {json.dumps({'text': text})}\n\n"
                    await asyncio.sleep(0)

                # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Œäº†å¾Œã«è‘—è€…æŠ½å‡ºã¨æ¤œç´¢ã‚’å®Ÿè¡Œ
                print("\nğŸ’¬ LLMå¿œç­”å®Œäº†ã€‚è‘—è€…æŠ½å‡ºã‚’é–‹å§‹...")

                # âœ… è‘—è€…æŠ½å‡ºå¯¾è±¡ï¼šuser/assistant ã®ã¿ + æ–°ã—ã„å¿œç­”
                updated_history = history + [{"role": "assistant", "content": collected_message}]
                plain_dialogue = [m for m in updated_history if m["role"] in ("user", "assistant")]

                # ğŸ“ æœ€æ–°ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—
                latest_user_message = ""
                user_messages = [m for m in plain_dialogue if m["role"] == "user"]
                if user_messages:
                    latest_user_message = user_messages[-1]["content"]

                total_user_turns = len(user_messages)

                # ğŸ“‹ ã‚¿ãƒ¼ãƒ³ç•ªå·ä»˜ãã®å±¥æ­´ã‚’ä½œæˆ
                numbered_dialogue = []
                user_turn = 0
                for m in plain_dialogue:
                    if m["role"] == "user":
                        user_turn += 1
                        numbered_dialogue.append(f"[ã‚¿ãƒ¼ãƒ³{user_turn}] user: {m['content']}")
                    else:
                        numbered_dialogue.append(f"assistant: {m['content']}")

                formatted_history = "\n".join(numbered_dialogue)

                print(f"\nğŸ” å¯¾è©±å±¥æ­´ï¼ˆã‚¿ãƒ¼ãƒ³ç•ªå·ä»˜ãï¼‰:")
                print(formatted_history)
                print(f"\nğŸ¯ æœ€æ–°ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆã‚¿ãƒ¼ãƒ³{total_user_turns}ï¼‰: ã€Œ{latest_user_message}ã€")

                # ğŸ¯ è‘—è€…åæŠ½å‡ºï¼ˆéã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰
                def author_extraction_sync():
                    return openai_client.chat.completions.create(
                        model="gpt-4o",
                        messages=[
                            {"role": "system", "content": f"""
ã‚ãªãŸã¯å¯¾è©±å±¥æ­´ã‹ã‚‰è‘—è€…åã‚’æŠ½å‡ºã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚

## å¯¾è©±ã®æ§‹é€ ã«ã¤ã„ã¦
- å¯¾è©±ã¯æ™‚ç³»åˆ—é †ã«ä¸¦ã‚“ã§ã„ã¾ã™ï¼ˆ[ã‚¿ãƒ¼ãƒ³1]ãŒæœ€åˆã€[ã‚¿ãƒ¼ãƒ³{total_user_turns}]ãŒæœ€æ–°ï¼‰
- ã‚¿ãƒ¼ãƒ³ç•ªå·ãŒå¤§ãã„ã»ã©æ–°ã—ã„ç™ºè¨€ã§ã™
- **[ã‚¿ãƒ¼ãƒ³{total_user_turns}]ãŒç¾åœ¨ã®æœ€æ–°ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè¨€**ã§ã™

## æŠ½å‡ºãƒ«ãƒ¼ãƒ«ï¼ˆå„ªå…ˆåº¦é †ï¼‰
1. **æœ€æ–°ã‚¿ãƒ¼ãƒ³ï¼ˆ[ã‚¿ãƒ¼ãƒ³{total_user_turns}]ï¼‰ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æœ€å„ªå…ˆ**ã§å‡¦ç†ã™ã‚‹
   - æœ€æ–°ã‚¿ãƒ¼ãƒ³ã§è‘—è€…åãŒæ˜ç¤ºã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€éå»ã®è¨€åŠã‚’ç„¡è¦–ã—ã¦ãã‚Œã‚’æ¡ç”¨
   - ã€Œã€œã•ã‚“ã€ã€Œã€œå…ˆç”Ÿã€ãªã©ã®æ•¬èªã‚‚è‘—è€…åã¨ã—ã¦èªè­˜
2. æœ€æ–°ã‚¿ãƒ¼ãƒ³ã§è‘—è€…ãŒä¸æ˜ãªå ´åˆã®ã¿ã€éå»ã®ã‚¿ãƒ¼ãƒ³ã‚’å‚è€ƒã«ã™ã‚‹
3. è¤‡æ•°ã®è‘—è€…ãŒç•°ãªã‚‹ã‚¿ãƒ¼ãƒ³ã§è¨€åŠã•ã‚ŒãŸå ´åˆã¯ã€**ã‚ˆã‚Šå¤§ããªã‚¿ãƒ¼ãƒ³ç•ªå·**ï¼ˆã‚ˆã‚Šæ–°ã—ã„ï¼‰ã‚’é¸ã¶
4. è‘—è€…åã®ã¿ã‚’æ—¥æœ¬èªã§æ­£ç¢ºã«è¿”ã™ï¼ˆæ–‡ç« ã§ã¯ãªãåå‰ã ã‘ï¼‰
5. ã©ã®ã‚¿ãƒ¼ãƒ³ã§ã‚‚æ˜ç¤ºã•ã‚Œã¦ã„ãªã‘ã‚Œã°ã€Œä¸æ˜ã€ã¨ã ã‘è¿”ã™

## æŠ½å‡ºä¾‹
- [ã‚¿ãƒ¼ãƒ³3] user: ã€Œæœ‰æ –å·æœ‰æ –ã•ã‚“ã€â†’ã€Œæœ‰æ –å·æœ‰æ –ã€
- [ã‚¿ãƒ¼ãƒ³2] user: ã€Œå¤ç›®æ¼±çŸ³ã«ã¤ã„ã¦æ•™ãˆã¦ã€â†’ã€Œå¤ç›®æ¼±çŸ³ã€
- [ã‚¿ãƒ¼ãƒ³1] user: ã€Œæ±é‡åœ­å¾ã€, [ã‚¿ãƒ¼ãƒ³2] user: ã€Œæœ‰æ –å·æœ‰æ –ã€â†’ã€Œæœ‰æ –å·æœ‰æ –ã€ï¼ˆæ–°ã—ã„ã‚¿ãƒ¼ãƒ³ã‚’å„ªå…ˆï¼‰

## ç¾åœ¨ã®çŠ¶æ³
- ç·ã‚¿ãƒ¼ãƒ³æ•°: {total_user_turns}ã‚¿ãƒ¼ãƒ³
- æœ€æ–°ï¼ˆæœ€ã‚‚é‡è¦ï¼‰ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: ã€Œ{latest_user_message}ã€
- **ã“ã®æœ€æ–°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆ[ã‚¿ãƒ¼ãƒ³{total_user_turns}]ï¼‰ã‹ã‚‰è‘—è€…åã‚’æœ€å„ªå…ˆã§æŠ½å‡ºã—ã¦ãã ã•ã„**

## å‡¦ç†æ–¹é‡
1. ã¾ãš[ã‚¿ãƒ¼ãƒ³{total_user_turns}]ã€Œ{latest_user_message}ã€ã‚’è©³ã—ãåˆ†æ
2. ã“ã“ã«è‘—è€…åãŒã‚ã‚Œã°å³åº§ã«æ¡ç”¨
3. ãªã‘ã‚Œã°ã‚¿ãƒ¼ãƒ³ç•ªå·ã®å¤§ãã„é †ï¼ˆæ–°ã—ã„é †ï¼‰ã«ç¢ºèª
"""},
                            {"role": "user", "content": formatted_history}
                        ],
                        temperature=0.1,  # ã‚ˆã‚Šä¸€è²«ã—ãŸçµæœã®ãŸã‚ä½ãè¨­å®š
                    )

                author_response = await loop.run_in_executor(None, author_extraction_sync)
                author_name = author_response.choices[0].message.content.strip()

                # æ•¬èªã‚’é™¤å»ã™ã‚‹å‡¦ç†
                author_name = author_name.replace("ã•ã‚“", "").replace("å…ˆç”Ÿ", "").replace("æ°", "")

                print(f"\nğŸ” æ¨å®šã•ã‚ŒãŸè‘—è€…å: [{author_name}]")

                # ğŸ” ã‚¸ãƒ£ãƒ‘ãƒ³ã‚µãƒ¼ãƒAPIã«ã‚ˆã‚‹æ¤œç´¢
                results = []
                if author_name.strip() != "ä¸æ˜":
                    def search_sync():
                        return search_japan_search_by_author(author_name)

                    results = await loop.run_in_executor(None, search_sync)
                    print(f"ğŸ“š æ¤œç´¢çµæœæ•°: {len(results)} ä»¶")

                # è‘—è€…åã¨æ¤œç´¢çµæœã‚’é€ä¿¡
                yield f"data: {json.dumps({'author': author_name, 'results': results})}\n\n"

            except Exception as e:
                print(f"\nâ— ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
                yield f"data: {json.dumps({'error': str(e)})}\n\n"

            yield f"data: {json.dumps({'done': True})}\n\n"

        return StreamingResponse(generator(), media_type="text/event-stream")

    except Exception as e:
        print(f"\nâ— ã‚¨ãƒ©ãƒ¼: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)
